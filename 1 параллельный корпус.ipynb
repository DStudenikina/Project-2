{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f31904e",
   "metadata": {},
   "source": [
    "Создание копуса с параллельными контекстами (ориганл + три перевода)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda98db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import razdel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fa43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Папка с файлами\n",
    "base_dir = r\"C:\\Users\\Sony\\OneDrive\\Рабочий стол\\Переподготовка ВШЭ\\проект\\2\\LOTR\\06.02\\3\" \n",
    "\n",
    "# Файлы\n",
    "en_file = os.path.join(base_dir, \"orig_a_3_1.txt\")  \n",
    "ru_files = [\n",
    "    os.path.join(base_dir, \"1_a_3_1.txt\"),   # Кистяковский Муравьев\n",
    "    os.path.join(base_dir, \"2_a_3_1.txt\"),   # Грушецкий Григорьева\n",
    "    os.path.join(base_dir, \"3_a_3_1.txt\")    # Каррик Каменкович\n",
    "]\n",
    "\n",
    "# Куда сохранить результат\n",
    "output_file = os.path.join(base_dir, \"par_corp_a_3_1.xlsx\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666af4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Английский: 35443 символов\n",
      "Перевод 1: 34037 символов\n",
      "Перевод 2: 28924 символов\n",
      "Перевод 3: 36322 символов\n"
     ]
    }
   ],
   "source": [
    "# Очистка текста\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Читаем английский\n",
    "with open(en_file, 'r', encoding='utf-8') as f:\n",
    "    en_text = preprocess_text(f.read())\n",
    "\n",
    "# Читаем русские переводы\n",
    "ru_texts = []\n",
    "for ru_file in ru_files:\n",
    "    with open(ru_file, 'r', encoding='utf-8') as f:\n",
    "        ru_texts.append(preprocess_text(f.read()))\n",
    "\n",
    "print(f\"Английский: {len(en_text)} символов\")\n",
    "for i, text in enumerate(ru_texts, 1):\n",
    "    print(f\"Перевод {i}: {len(text)} символов\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a14dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Перевод 1: 365 предложений\n",
      "  Перевод 2: 414 предложений\n",
      "  Перевод 3: 508 предложений\n",
      "Английский: 275 предложений\n"
     ]
    }
   ],
   "source": [
    "# Разделяем на предложения\n",
    "\n",
    "def split_english_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-ZА-Я])', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Английские предложения\n",
    "en_sentences = split_english_sentences(en_text)\n",
    "\n",
    "# Русские предложения\n",
    "ru_sentences_list = []\n",
    "for i, ru_text in enumerate(ru_texts, 1):\n",
    "    sentences = [sentence.text for sentence in razdel.sentenize(ru_text)]\n",
    "    ru_sentences_list.append(sentences)\n",
    "    print(f\"  Перевод {i}: {len(sentences)} предложений\")\n",
    "\n",
    "\n",
    "print(f\"Английский: {len(en_sentences)} предложений\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d68522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [01:28<00:00,  9.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перевод 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [02:05<00:00, 10.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перевод 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13/13 [01:49<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перевод 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [02:08<00:00,  8.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Создаем эмбеддинги\n",
    "\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# Эмбеддинги для английского\n",
    "en_vectors = model.encode(en_sentences, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Эмбеддинги для русских переводов\n",
    "ru_vectors_list = []\n",
    "for i, ru_sentences in enumerate(ru_sentences_list, 1):\n",
    "    print(f\"Перевод {i}...\")\n",
    "    vectors = model.encode(ru_sentences, batch_size=32, show_progress_bar=True)\n",
    "    ru_vectors_list.append(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Поиск соответствий: 100%|██████████| 452/452 [00:01<00:00, 235.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 124 пар\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Выравниваем с первым переводом (КистяМур)\n",
    "\n",
    "alignments = []\n",
    "used_ru = set()\n",
    "\n",
    "# Берем первый перевод как базовый\n",
    "ru_sentences_1 = ru_sentences_list[0]\n",
    "ru_vectors_1 = ru_vectors_list[0]\n",
    "\n",
    "for i in tqdm(range(len(en_sentences)), desc=\"Поиск соответствий\"):\n",
    "    best_j = -1\n",
    "    best_score = 0\n",
    "    \n",
    "    # Определяем окно поиска\n",
    "    k = len(en_sentences) / len(ru_sentences_1)\n",
    "    j_start = max(0, int((i - 20) / k))\n",
    "    j_end = min(len(ru_sentences_1), int((i + 20) / k) + 1)\n",
    "    \n",
    "    # Ищем лучшее соответствие\n",
    "    for j in range(j_start, j_end):\n",
    "        if j in used_ru:\n",
    "            continue\n",
    "        \n",
    "        sim = 1 - spatial.distance.cosine(en_vectors[i], ru_vectors_1[j])\n",
    "        if sim > best_score:\n",
    "            best_score = sim\n",
    "            best_j = j\n",
    "    \n",
    "    # Если нашли хорошее соответствие\n",
    "    if best_score >= 0.65 and best_j != -1:\n",
    "        alignments.append({\n",
    "            'pair_id': len(alignments) + 1,\n",
    "            'en_idx': i,\n",
    "            'english': en_sentences[i],\n",
    "            'russian_1': ru_sentences_1[best_j],\n",
    "            'sim_1': best_score\n",
    "        })\n",
    "        used_ru.add(best_j)\n",
    "\n",
    "print(f\"Найдено {len(alignments)} пар\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd44223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка пар: 100%|██████████| 124/124 [00:01<00:00, 121.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ищем соответствия в других переводах\n",
    "\n",
    "# Имена переводчиков\n",
    "translator_names = [\"2\", \"3\"]\n",
    "\n",
    "for align in tqdm(alignments, desc=\"Обработка пар\"):\n",
    "    en_idx = align['en_idx']\n",
    "    \n",
    "    # Для каждого перевода (кроме первого)\n",
    "    for trans_idx in range(1, 3):\n",
    "        ru_sentences = ru_sentences_list[trans_idx]\n",
    "        ru_vectors = ru_vectors_list[trans_idx]\n",
    "        trans_name = translator_names[trans_idx-1]\n",
    "        \n",
    "        best_j = -1\n",
    "        best_score = 0\n",
    "        \n",
    "        # Определяем окно поиска\n",
    "        k = len(en_sentences) / len(ru_sentences)\n",
    "        j_start = max(0, int((en_idx - 20) / k))\n",
    "        j_end = min(len(ru_sentences), int((en_idx + 20) / k) + 1)\n",
    "        \n",
    "        # Ищем лучшее соответствие\n",
    "        for j in range(j_start, j_end):\n",
    "            sim = 1 - spatial.distance.cosine(en_vectors[en_idx], ru_vectors[j])\n",
    "            if sim > best_score:\n",
    "                best_score = sim\n",
    "                best_j = j\n",
    "        \n",
    "        # Добавляем в результат\n",
    "        align[f'russian_{trans_name}'] = ru_sentences[best_j] if best_j != -1 else ''\n",
    "        align[f'sim_{trans_name}'] = best_score if best_j != -1 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataFrame\n",
    "data = []\n",
    "for align in alignments:\n",
    "    row = {\n",
    "        'pair_id': align['pair_id'],\n",
    "        'english': align['english'],\n",
    "        'russian_1': align['russian_1'],\n",
    "        'russian_2': align.get('russian_2', ''),\n",
    "        'russian_3': align.get('russian_3', ''),\n",
    "        'sim_1': align.get('sim_1', 0),\n",
    "        'sim_2': align.get('sim_2', 0),\n",
    "        'sim_3': align.get('sim_3', 0)\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Сохраняем в Excel\n",
    "df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создано параллельных контекстов: 124\n",
      "Сохранено в: C:\\Users\\Sony\\OneDrive\\Рабочий стол\\Переподготовка ВШЭ\\проект\\2\\LOTR\\06.02\\2\\par_corp_a_2_1.xlsx\n",
      "\n",
      "Первые 3 строки:\n",
      "   pair_id                                            english  \\\n",
      "0        1  Two or three weeks had passed, and still Frodo...   \n",
      "1        2                         I am getting very anxious.   \n",
      "2        3  But you are leaving the Shire — and that shoul...   \n",
      "\n",
      "                                           russian_1  \n",
      "0  С первого разговора прошло уже две или три нед...  \n",
      "1                         Что-то мне очень тревожно.  \n",
      "2  Но Хоббитанию ты покидаешь – и надо, чтобы об ...  \n",
      "\n",
      "Колонки в файле:\n",
      "['pair_id', 'english', 'russian_1', 'russian_2', 'russian_3', 'sim_1', 'sim_2', 'sim_3']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Создано параллельных контекстов: {len(df)}\")\n",
    "print(f\"Сохранено в: {output_file}\")\n",
    "\n",
    "print(\"\\nПервые 3 строки:\")\n",
    "print(df[['pair_id', 'english', 'russian_1']].head(3))\n",
    "\n",
    "print(\"\\nКолонки в файле:\")\n",
    "print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bffcaf",
   "metadata": {},
   "source": [
    "Делаем то же самое с другими главами"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
